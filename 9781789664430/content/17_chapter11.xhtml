<?xml version="1.0"?>
<html xmlns:epub="http://www.idpf.org/2007/ops" xmlns="http://www.w3.org/1999/xhtml" lang="en-GB" xml:lang="en-GB">
<head>
<title>11 Optimizing the optimization</title>
<meta charset="UTF-8"/>
<link href="../koganpage.css" rel="stylesheet" type="text/css"/>
</head>
<body><section id="ch11_sec_001" class="chapter" epub:type="chapter" role="doc-chapter" aria-labelledby="ch11_h1_001">
<header>
<h1 id="ch11_h1_001" class="title" epub:type="title"><span id="page-196" role="doc-pagebreak" aria-label=" Page 196. " epub:type="pagebreak"/><span class="ordinal number" epub:type="ordinal" id="ch11_num_001">11</span> Optimizing the optimization</h1>
</header>
<p class="text-fo">Kaizen is an approach to continuous improvement, where small changes are constantly introduced to improve quality and efficiency. You can apply the Kaizen mindset to optimize your own optimization process.</p>
<p class="text">One way to assess overall performance of the optimization programme is compounded annual uplift. This takes into account the effect of all your winning tests over the course of a year. You can leverage big gains in your annual results by making small changes to three key levers. These power metrics hold the key to exponentially improve the ROI of your entire optimization programme.</p>
<p class="text">The three power metrics are:</p>
<ol class="list-number"><li><p class="list-number"><b>Test velocity</b>: How many experiments launched in a given period, usually measured as tests per year.</p></li>
<li><p class="list-number"><b>Win rate</b>: The number of tests that generated a positive uplift, expressed as a percentage of the total.</p></li>
<li><p class="list-number"><b>Average uplift</b>: The average size of uplifts of winning tests, in terms of RPV or another KPI.</p></li>
</ol>
<p class="text-fo">Here&#x2019;s how it works in practice. Let&#x2019;s say this is your baseline performance:</p>
<figure class="table"><p class="table-skip"><a href="#ch11_skiptab_001">Skip table</a></p>
<!--<div class="table" role="table" tabindex="0">--><table class="shade-odd left">
<tbody><tr>
<th scope="row"><p class="table-text">Test velocity over the last 12 months</p></th>
<td><p class="table-text">24 tests</p></td>
</tr>
<tr>
<th scope="row" class="no-shade"><p class="table-text">Win rate</p></th>
<td><p class="table-text">29% (7 tests were positive, 17 negative)</p></td>
</tr>
<tr>
<th scope="row"><p class="table-text">Average uplift per test</p></th>
<td><p class="table-text">5.9%</p></td>
</tr></tbody></table>
<!--</div>--></figure>
<p id="ch11_skiptab_001" class="text-fo">Compounding the effects of the seven positive tests, averaging 5.9 per cent RPV uplift, generates a total compounded site-wide uplift, over the last 12 months, of 49 per cent.</p>
<p class="text">However, don&#x2019;t fall into the trap of thinking that this means a 49 per cent increase in sales. Extrapolating split-test results into predictable sales increases is notoriously difficult. For one, you have to calculate the impact of <span id="page-197" role="doc-pagebreak" aria-label=" Page 197. " epub:type="pagebreak"/>that uplift on site-wide revenue, covered in <a href="13_chapter07.xhtml#ch07_sec_001">Chapters 7</a> and <a href="15_chapter09.xhtml#ch09_sec_001">9</a>. We also saw that 95 per cent statistical significance means that 1 in 20 wins is a fluke.</p>
<p class="text">A test measures short-term changes in user behaviour. Your test may run for a few weeks, reflecting a certain level of increase in RPV. After you have coded the experiment to be a permanent part of your site, the gain may not be exactly the same as that observed during the test. There are other factors at play, including long-term changes in customer behaviour, competitor actions and even the economy. This may cause fluctuations.</p>
<p class="text">To be clear: you will have the benefit of a win after it&#x2019;s gone live on the site. What you can&#x2019;t be certain about is the precise effect on an ongoing basis, without running a holdback set. A holdback set is when you continue to serve the control to a small segment, typically 5 per cent, while 95 per cent of your visitor base will see the winning variation. You can only do this if you have huge volumes of traffic and a platform designed to work this way. Even then, the value in this practice is debatable (see <a href="15_chapter09.xhtml#ch09_sec_001">Chapter 9</a> for more).</p>
<p class="text">These factors don&#x2019;t take anything away from the fact that you are growing online revenue in demonstrable ways. Referring to the <b>ROI of experimentation program</b><b>me</b><b>s</b>, Harvard Business School Professor Stefan Thomke says: &#x2018;What&#x2019;s the cost of NOT doing it?&#x2019;<sup><a href="#ch11_en1" id="ch11_enx1">1</a></sup></p>
<p class="text">So the value of the compounded annual uplift lies in being a robust benchmark of your optimization programme. To improve it, focus on those three power metrics:</p>
<ul class="list-bullet"><li><p class="list-bullet">Run more tests.</p></li>
<li><p class="list-bullet">Get more of those tests to win.</p></li>
<li><p class="list-bullet">Increase your average magnitude of lift per winning test.</p></li>
</ul>
<p class="text-fo"><a id="rch11_tab_001" href="#ch11_tab_001">Table 11.1</a> illustrates the impact of an increase of just 10 per cent in each area.</p>
<figure class="table" id="ch11_tab_001"><figcaption>
<p class="table"><a href="#rch11_tab_001"><span class="label" epub:type="label">Table</span> <span class="ordinal" epub:type="ordinal">11.1</span> Thirty-two per cent improvement in total uplift from just a 10 per cent increase in each of your three power metrics</a></p>
</figcaption><p class="table-skip"><a href="#ch11_tabskip_001">Skip table</a></p>
<!--<div class="table" role="table" tabindex="0">--><table class="shade-even">
<thead><tr>
<td><p class="table-head"/></td>
<th scope="col"><p class="table-head"><b>Benchmark</b></p></th>
<th scope="col"><p class="table-head"><b>10% improvement</b></p></th>
</tr></thead>
<tbody><tr>
<td><p class="table-text">Test velocity in 12 months</p></td>
<td><p class="table-text">24</p></td>
<td><p class="table-text">26</p></td>
</tr>
<tr>
<td><p class="table-text">Win rate</p></td>
<td><p class="table-text">29%</p></td>
<td><p class="table-text">32%</p></td>
</tr>
<tr>
<td><p class="table-text">Average uplift</p></td>
<td><p class="table-text">5.86%</p></td>
<td><p class="table-text">6.44%</p></td>
</tr>
<tr>
<td><p class="table-text">&#x00A0;</p></td>
<td><p class="table-text"/></td>
<td><p class="table-text"/></td>
</tr>
<tr>
<td><p class="table-text">Compounded uplift over 12 months</p></td>
<td><p class="table-text">49%</p></td>
<td><p class="table-text">64%</p></td>
</tr></tbody></table><!--</div>--></figure>
<p id="ch11_tabskip_001" class="text-fo"><span id="page-198" role="doc-pagebreak" aria-label=" Page 198. " epub:type="pagebreak"/>You can see that those tiny changes in each area have had a profound effect on the compounded annual uplift, which has gone up from 49 per cent to 64 per cent. This is a healthy indicator that your optimization programme is benefiting from being optimized itself.</p>
<p class="text">Below are some ways in which you can improve the three metrics.</p>
<section aria-labelledby="ch11_h3_001">
<h2 id="ch11_h3_001" class="head-a">Power metric 1: How to increase test velocity</h2>
<p class="text-fo">The truth is, you have very little to no control over win rate and average uplift. However, velocity is almost completely within your control. Therefore, this should be your main lever. In terms of experimentation programme success metrics, this is the one that matters.</p>
<p class="text">However, it does not mean simply increasing the number of tests for the sake of it, throwing spaghetti at the ceiling to see what sticks. Indulging in random acts of testing (RATs) to increase velocity will depress your win rate and average uplift, and ultimately reduce the ROI on your optimization programme.</p>
<p class="text">Five strategies you can use, without sacrificing the quality of your testing programme, are:</p>
<ol class="list-number"><li><p class="list-number">Keep filling your bucket of new ideas from the pipeline, so there&#x2019;s never a time when you are searching around for a hypothesis to test. This can be done by continuously mining relevant data sources, for example running regular small batches of usability testing. New ideas and hypotheses also come from post-test analysis, covered in <a href="15_chapter09.xhtml#ch09_sec_001">Chapter 9</a>.</p></li>
<li><p class="list-number">Work out your testing capacity and how many testing slots you have on your website. As explained in <a href="13_chapter07.xhtml#ch07_sec_001">Chapter 7</a>, these may include:</p>
<ul class="list-bullet-sub">
<li><p class="list-number">Homepage</p></li>
<li><p class="list-number">Category page</p></li>
<li><p class="list-number">Product listing page (PLP)</p></li>
<li><p class="list-number">Product detail page (PDP)</p></li>
<li><p class="list-number">Search results page</p></li>
<li><p class="list-number">Basket page</p></li>
<li><p class="list-number">Checkout pages</p></li>
<li><p class="list-number">Site-wide (header, navigation etc).</p></li>
</ul>
<p class="list-number">Think of your tests like spaces in a car park. If you have a test running on one area, that is one of your car park spaces occupied. You won&#x2019;t be able to run another test on this area until the first one has been declared.</p></li>
<li><p class="list-number"><span id="page-199" role="doc-pagebreak" aria-label=" Page 199. " epub:type="pagebreak"/>For each testing slot, have one waiting in the wings, queued up and ready to go. To minimize the risk of cross-contamination, avoid running concurrent tests that might have an effect on each other.</p></li>
<li><p class="list-number">Executing complex tests takes time and money. At the same time, bolder tests are more likely to result in big swings that are easier to detect. Try to find a balance between the two on your roadmap. If there are already tests in progress, leaving your developer with spare capacity, it&#x2019;s time to tackle something with more complexity.</p></li>
<li><p class="list-number">Run your tests only for the required amount of time. Don&#x2019;t extend test duration without good reason. If you leave them running longer than necessary, you are occupying one of your car parking spaces, and reducing the efficiency of your optimization programme. As discussed in <a href="15_chapter09.xhtml#ch09_sec_001">Chapter 9</a>, the industry standard 95 per cent statistical significance is not an absolute requirement. Watch the other signals and, if necessary, settle on a lower level in exchange for moving faster.</p></li>
</ol>
<p class="text-fo">Calculate in advance how long a test should run, adjusting the minimum detectable effect (MDE) as explained in <a href="13_chapter07.xhtml#ch07_sec_001">Chapters 7</a> and <a href="15_chapter09.xhtml#ch09_sec_001">9</a>. Increasing the MDE will reduce the length of time required to reach significance. For example, if an MDE of 3 per cent will take too long, increase it to 5 per cent or higher. This means you won&#x2019;t detect results that are less than 5 per cent and means you can make the best use of your available traffic and testing slots.</p>
<section aria-labelledby="ch11_h3_002">
<h3 id="ch11_h3_002" class="head-b">Power metric 2: How to increase your win rate</h3>
<p class="text-fo">If 100 per cent of your split tests are winning, you either had a very poor website to begin with, or you are misinterpreting test results and declaring false positives. False positives look like wins, but on closer inspection there is unlikely to be any real change to visitor behaviour. In fact, sometimes it could even have a have a damaging impact on sales.</p>
<p class="text">Likewise, if only a tiny percentage of your tests are beating the control, then you also have a problem. Don&#x2019;t obsess over what that number should be. The most important thing is to get your overall win rate increasing from its baseline.</p>
<p class="text">The fact is, every test has a 50/50 chance of winning or losing. In a famous letter to Amazon shareholders, Jeff Bezos wrote &#x2018;if you know in advance that it&#x2019;s going to work, it&#x2019;s not an experiment&#x2019;.<sup><a epub:type="noteref" role="doc-noteref" href="#ch11_en2" id="ch11_enx2">2</a></sup></p>
<p class="text"><span id="page-200" role="doc-pagebreak" aria-label=" Page 200. " epub:type="pagebreak"/>That said, here are approaches to increase your chances of seeing a higher win rate:</p>
<ol class="list-number"><li><p class="list-number">Make sure your hypothesis development is informed by data and research, so that you avoid RATs.</p></li>
<li><p class="list-number">Prioritize your hypotheses diligently &#x2013; if you prioritize dumb tests, then however good your creative execution is, win rate will be less than you could have achieved.</p></li>
<li><p class="list-number">Have more than one variation per test &#x2013; this increases the number of false positives, but it&#x2019;s a small price to pay for increasing discovery rate, quality of insights and more real wins too. The only caveat is that it will extend test duration, so use this tactic wisely if you have enough traffic.</p></li>
</ol>
<p class="text-fo">To avoid any misunderstanding, a higher velocity is more important than a higher win rate. If for no other reason, you have more control over the number of tests.</p>
</section>
<section aria-labelledby="ch11_h3_003">
<h3 id="ch11_h3_003" class="head-b">Power metric 3: How to increase your average uplift</h3>
<p class="text-fo">You can&#x2019;t predict the outcome of an experiment, let alone the potential size of uplift. By observing these best practices, you can give yourself the best chance:</p>
<ol class="list-number"><li><p class="list-number">Test bolder ideas. Moving the needle is easier when you test noticeable changes. Remember, it can swing both ways &#x2013; up or down. It also requires more development time. That doesn&#x2019;t matter. One win can pay for all those losses, as we explained in <a href="15_chapter09.xhtml#ch09_sec_001">Chapter 9</a>.</p></li>
<li><p class="list-number">Review the results from previous tests. This will help you to identify areas where you are consistently getting high-impact results, or areas with significant potential.</p></li>
<li><p class="list-number">Refine your approach to prioritization. Examine which tests produced healthy uplifts and see if there are any patterns. Also look at tests that didn&#x2019;t produce a positive result and consider what you can learn about how this should shape your prioritization.</p></li>
</ol>
</section>
</section>
<section aria-labelledby="ch11_h2_001">
<h2 id="ch11_h2_001" class="head-a">Summary</h2>
<p class="text-fo">The process of optimization is itself a target for optimization. You can improve your experimentation programme exponentially by focusing on three <span id="page-201" role="doc-pagebreak" aria-label=" Page 201. " epub:type="pagebreak"/>key metrics. An increase in any one of these areas will improve your annual compounded uplift:</p>
<ol class="list-number"><li><p class="list-number">Test velocity &#x2013; the number of tests you run over the period, usually a year</p></li>
<li><p class="list-number">Win rate &#x2013; the number of tests that show a positive uplift</p></li>
<li><p class="list-number">Average uplift</p></li>
</ol>
<p class="text-fo">The one metric that matters is velocity, because it is the one most within your control. Ensure that you are testing on as many areas of the website as possible and that you have a new test ready to go as soon as each split test is declared. Don&#x2019;t extend the run of a test in the hope of it &#x2018;turning around&#x2019;. The industry standard 95 per cent statistical significance is not an absolute requirement, so you may accept a lower level in exchange for moving faster.</p>
<p class="text">You have very little to no control over win rate and average uplift, but best practices will improve your chances. Most of them are rooted in diligent research, analysis and prioritization, as well as careful planning of logistics.</p>
<p class="text">An increase in all three will optimize your optimization exponentially. This is not a measure of how much your revenue will increase, but a robust way to benchmark the fundamental effectiveness of your CRO programme.</p>
</section>
<section id="ch11_sec_002" class="chapter-endnotes" aria-labelledby="ch11_h2_002" epub:type="endnotes" role="doc-endnotes">
<h2 id="ch11_h2_002" class="title" epub:type="title">Notes</h2>
<ol class="none">
<li class="endnote-text" id="ch11_en1"><a role="doc-backlink"  href="#ch11_enx1">1</a> Optimizely (2018) [Accessed 7 August 2020] Optimizely partner story: HBS Professor Stefan Thomke on Experimentation [Video interview], <i>YouTube</i> [Online] <a href="https://youtu.be/5sklu7J5r90"><span class="url-hyperlink">https://youtu.be/5sklu7J5r90</span></a> (archived at <a href="https://perma.cc/L5ZP-UGZQ"><span class="url-hyperlink">https://perma.cc/L5ZP-UGZQ</span></a>)</li>
<li class="endnote-text" id="ch11_en2"><a role="doc-backlink"  href="#ch11_enx2">2</a> Bezos, J (1997) [Accessed 7 August 2020] <a href="https://Amazon.com"><span class="url-hyperlink">Amazon.com</span></a> letter to shareholders, <i>US Securities and Exchange Commission</i> [Online] <a href="http://www.sec.gov/Archives/edgar/data/1018724/000119312516530910/d168744dex991.htm"><span class="url-hyperlink">www.sec.gov/Archives/edgar/data/1018724/000119312516530910/d168744dex991.htm</span></a> (archived at <a href="https://perma.cc/4M4X-UNHN"><span class="url-hyperlink">https://perma.cc/4M4X-UNHN</span></a>)</li>
</ol>

</section>
</section></body>
</html>