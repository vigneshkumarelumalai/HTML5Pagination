<?xml version="1.0"?>
<html xmlns:epub="http://www.idpf.org/2007/ops" xmlns="http://www.w3.org/1999/xhtml" lang="en-GB" xml:lang="en-GB">
<head>
<title>07 Create an experimentation roadmap</title>
<meta charset="UTF-8"/>
<link href="../koganpage.css" rel="stylesheet" type="text/css"/>
</head>
<body><section id="ch07_sec_001" class="chapter" epub:type="chapter" role="doc-chapter" aria-labelledby="ch07_h1_001">
<header>
<h1 id="ch07_h1_001" class="title" epub:type="title"><span id="page-117" role="doc-pagebreak" aria-label=" Page 117. " epub:type="pagebreak"/><span class="ordinal number" id="ch07_num_001" epub:type="ordinal">07</span> Create an experimentation roadmap</h1>
</header>
<p class="text-fo">In the early days of Amazon, experimentation was democratized throughout the organization. Almost anyone could test almost anything. Feel like changing a button colour today? If you had access to the platform, you could make it happen. There was a flood of dumb experiments.</p>
<p class="text">An ex-Amazon senior staffer recalls: &#x2018;(The tests) had no chance of yielding any value. There wasn&#x2019;t any point to them. We were just kind of curious. We were just running a lot of experiments, which have a cost by the way, and taking up experimental slots&#x2026;&#x2019;<sup><a href="#ch07_en1" id="ch07_enx1">1</a></sup></p>
<p class="text">They were throwing mud against the wall, seeing if any of it would stick. In the industry this is jokingly referred to as RATs &#x2013; Random Acts of Testing. It is surprisingly common, even today. In case there is any doubt at this point, it&#x2019;s a waste of time and resources. RATs will devour your budget with nothing to show but lots of testing activity, meaning negative ROI.</p>
<p class="text">High-ROI teams use logic to determine what gets tested when.</p>
<section aria-labelledby="ch07_h2_001">
<h2 id="ch07_h2_001" class="head-a">How to prioritize test ideas</h2>
<p class="text-fo">Coming up with ideas to test is easy. The trickier part is deciding which ones to start with, and which ones to relegate to the bottom of the list. That is prioritization.</p>
<p class="text">If it sounds like a purely operational endeavour, it&#x2019;s not. Fundamentally prioritization is about ROI. To quote Bill Gates: &#x2018;Prioritization is effectiveness.&#x2019;<sup><a href="#ch07_en2" id="ch07_enx2">2</a></sup></p>
<p class="text">On optimization programmes, prioritization generally happens on two levels. First, there is <b>top-down prioritization</b>, which aligns the programme with strategic focus areas. This does not mean that management get to decide the final order of tests, or whether something gets tested. It&#x2019;s guiding the team to work on things that matter from a business perspective.</p>
<p class="text"><span id="page-118" role="doc-pagebreak" aria-label=" Page 118. " epub:type="pagebreak"/>As an example, one of our agency clients asked us to focus on two product categories. It formed part of a broader initiative designed at corporate level to improve gross margins and profitability. For another client we honed in on mobile customer journeys, in line with their strategic agenda for that year.</p>
<p class="text"><b>Bottom-up prioritization</b> is performed by the team on a day-to-day basis to separate the wheat from the chaff. It relies on set rules and data points to score ideas relative to one another. Opinion, emotion and intuition are more or less stripped away.</p>
<p class="text">At the start of an experimentation programme, an unrestricted bottom-up view can be useful. It will help you to identify areas with the biggest promise in terms of ROI, which could shape any top-down view.</p>
<section aria-labelledby="ch07_h3_001">
<h3 id="ch07_h3_001" class="head-b">Value/Effort</h3>
<p class="text-fo">The most basic approximation for ROI is value divided by effort. Plotted as a matrix as shown in <a id="rch07_fig_001" href="#ch07_fig_001">Figure 7.1</a>, ideas with the highest ROI can be easily identified.</p>
<figure class="figure" id="ch07_fig_001"><figcaption>
<p class="figure"><a href="#rch07_fig_001"><span class="label" epub:type="label">Figure</span> <span class="ordinal" epub:type="ordinal">7.1</span> A basic Value/Effort prioritization matrix. Ideas with the highest ROI sit in the top left quadrant. In optimization, these are experiments that cost less to execute yet return strong increases in online sales</a></p>
</figcaption>
<p class="figure-media"><img src="images/M07NF001.jpg" alt="A diagram shows value or effort prioritization matrix from low to high. High value with low effort shows hell, yes!; high value and effort shows yes; low value and effort shows maybe!; and low value with high effort shows no!"/></p>
</figure>
<section>
<h4 class="head-c">Value</h4>
<p class="text-fo">It should come as no surprise that two perspectives are important when thinking about value:</p>
<ul class="list-bullet"><li><p class="list-bullet">value to your users;</p></li>
<li><p class="list-bullet">value to the business.</p></li>
</ul>
<p class="text-fo"><span id="page-119" role="doc-pagebreak" aria-label=" Page 119. " epub:type="pagebreak"/>The observations and insights in your evidence file (see <a href="09_chapter03.xhtml#ch03_sec_001">Chapter 3</a>) should by now be packed with ideas about how to improve the experience for users and customers. When you test these ideas, you&#x2019;d want to monitor changes in user behaviour.</p>
<p class="text">The test should also be set up to validate business value, by measuring the impact on KPIs (key performance indicators). The outcome of the experiment is usually judged against an appropriate metric. There is more on this in <a href="15_chapter09.xhtml#ch09_sec_001">Chapter 9</a>.</p>
</section>
<section>
<h4 class="head-c">Impact</h4>
<p class="text-fo">Your tests should directly impact business KPIs in a measurable way. For example, it&#x2019;s unlikely that a test on the About Us page would impact revenue directly. One of the factors in determining value is therefore the extent to which a test is likely to directly impact KPIs such as revenue.</p>
<p class="text">To measure the business value of an experiment, core e-commerce metrics are tracked. These are generally average order value (AOV) and sales conversion rate, which together make up revenue per visitor (RPV). In <a href="15_chapter09.xhtml#ch09_sec_001">Chapter 9</a>, we&#x2019;ll discuss this in more detail.</p>
<p class="text">Detecting a confident win tracking conversion rate is easier than using a revenue-based metric, such as AOV or RPV. This is because conversion rate is binary &#x2013; there are only two possibilities (either a user converted, or not), whereas revenue can vary greatly. The implication is that it may not be feasible to directly measure the impact of a test on revenue. You would then default to sales conversion rate.</p>
<p class="text">Even that could prove tough under certain conditions. The further a test is located from the final conversion point, the less feasible it is to use conversion rate as a metric. Then you have to rely on a <i>proxy metric</i>, ideally a user-based metric that is positively correlated with a core business metric. For example, clicks on the &#x2018;Add to Cart&#x2019; button could be a valid proxy metric for conversion rate. It shows purchase intent, even if the user didn&#x2019;t buy after adding the item to their basket.</p>
<p class="text">An approach often used on low-traffic sites is to track visits to the next page in the funnel. For example, a test on the PLP might track visits to the PDP as a measure of how effective the test is. The idea is that the sales funnel is more effective in driving users towards the final conversion point.</p>
<p class="text">Tests containing changes that impact sales directly should score higher than those that don&#x2019;t. Tests that allow you to track core metrics should score higher than those where you would have to rely on proxy metrics.</p>
<p class="text"><span id="page-120" role="doc-pagebreak" aria-label=" Page 120. " epub:type="pagebreak"/>A retailer we work with has a blog that receives enormous volumes of organic search traffic. It&#x2019;s a great customer acquisition tool. That said, only a small portion of blog readers ever become paying customers. When it happens, it&#x2019;s usually a nurturing process spanning weeks or months.</p>
<p class="text">While it certainly has a positive effect on conversion rate and revenue, the link is so tenuous that it can&#x2019;t be directly measured in experiments. Instead of e-commerce metrics, on those tests we track engagement and lead generation. Did more blog readers visit the online shop? Was there an increase in the number of e-mail addresses captured?</p>
<p class="text">Naturally, those experiments that track user-based proxy metrics, instead of core e-commerce metrics, are assigned lower priority.</p>
</section>
<section>
<h4 class="head-c">Value by testing slot</h4>
<p class="text-fo">Your tests are like spaces in a car park. If you have a test running on one area, that is one of your car park spaces occupied. You won&#x2019;t be able to run another test on this area until the first one has been declared.</p>
<p class="text">A testing slot is an area where experiments can be run. There is a finite number of testing slots, which makes it a scarce resource. The easiest way to think of testing slots on an e-commerce site is to split it up into different parts of the funnel. Typically, this is how it breaks down:</p>
<ul class="list-bullet"><li><p class="list-bullet">homepage;</p></li>
<li><p class="list-bullet">category pages;</p></li>
<li><p class="list-bullet">product listings pages;</p></li>
<li><p class="list-bullet">product details pages;</p></li>
<li><p class="list-bullet">basket page;</p></li>
<li><p class="list-bullet">checkout;</p></li>
<li><p class="list-bullet">about us;</p></li>
<li><p class="list-bullet">main menu;</p></li>
<li><p class="list-bullet">site search;</p></li>
<li><p class="list-bullet">header;</p></li>
<li><p class="list-bullet">footer;</p></li>
<li><p class="list-bullet">content/blog pages.</p></li>
</ul>
<p class="text-fo">Before even considering the merits of individual test ideas, the relative value of each testing slot can be determined by comparing the testable population and the revenue multipliers.</p>
</section>
<section>
<h4 class="head-c"><span id="page-121" role="doc-pagebreak" aria-label=" Page 121. " epub:type="pagebreak"/>Population</h4>
<p class="text-fo">In <a href="15_chapter09.xhtml#ch09_sec_001">Chapter 9</a>, we discuss the importance of sample size when calculating the outcome of a test. For now, accept that the higher the number of visitors (sample) in our test, the easier it is to be confident about the outcome. So the more visitors a page receives, the faster an experiment can be concluded. It follows that the most frequently visited pages present a more solid testing ground as a rule.</p>
<p class="text">Clustering pages together for experimentation purposes can ramp up sample size dramatically. It&#x2019;s common practice to test at template level rather than targeting individual pages. For example, by grouping together all the product detail pages (PDPs), the sample size is the sum of all the individual PDP visitors. On a site that sells cameras and lenses, you might have a separate grouping for each product category. The more granular, the smaller the sample size.</p>
<p class="text">The area of the page where the change is introduced can also play a role. Say you want to treat an element in the lower half of a page. Google Analytics counts 100,000 visitors on that page, but not everyone scrolls down far enough to see this change. If only 20 per cent of those visitors scroll down to the area where the change is located, the available population for this experiment is sliced from 100,000 to 20,000 visitors. Scrollmaps, as discussed in <a href="10_chapter04.xhtml#ch04_sec_001">Chapter 4</a>, can give you this perspective.</p>
<p class="text">Site-wide elements, like the header and main navigation, offer a comparatively large test population. Nearly all visitors see the global header, only some will see a category page, and even fewer will make it to the PDP. However, there is a trade-off with &#x2018;Impact&#x2019;, as you&#x2019;ll see later in this section.</p>
</section>
<section>
<h4 class="head-c">Revenue multiplier</h4>
<p class="text-fo">In scenario B below, a 10 per cent increase in sales is worth double that of scenario A. All things being equal, it makes sense to give priority to the opportunity with the bigger upside.</p>
<ol class="list-number"><li><p class="list-number">A: Baseline $100,000. 10 per cent increase = $10,000</p></li>
<li><p class="list-number">B: Baseline $200,000. 10 per cent increase = $20,000</p></li>
</ol>
<p class="text-fo">The closer to the checkout, the higher the monetary value of each percentage point lift, as more revenue passes through that part of the funnel. To be clear, this does not mean that the checkout should be higher priority than other pages by default. For one, the checkout has a lot less visitor traffic than higher up the funnel.</p>
<p class="text"><span id="page-122" role="doc-pagebreak" aria-label=" Page 122. " epub:type="pagebreak"/>Use Google Analytics to calculate and compare the revenue multiplier for each testing slot. To do this, create an advanced segment as explained in <a href="10_chapter04.xhtml#ch04_sec_001">Chapter 4</a> and compare this segment against <i>All Sessions</i> in the <i>E-commerce Overview</i> report.</p>
<p class="text">This should give you a view like the one in <a id="rch07_fig_002" href="#ch07_fig_002">Figure 7.2</a>. In this example, homepage viewers contribute roughly 32 per cent to total site revenue, so that is the multiplier for this page. A 10 per cent increase in sales on the homepage is actually worth only 3.2 per cent (10 per cent increase &#x00D7; 32 per cent revenue contribution = 3.2 per cent site-wide impact). In contrast, PDP viewers contribute roughly 80 per cent to overall revenue, so a 10 per cent uplift on PDP would translate into 8 per cent site-wide.</p>
<figure class="figure" id="ch07_fig_002"><figcaption>
<p class="figure"><a href="#rch07_fig_002"><span class="label" epub:type="label">Figure</span> <span class="ordinal" epub:type="ordinal">7.2</span> Using GA advanced segments to compare revenue for homepage views to all sessions reveals a multiplier of 32 per cent That&#x2019;s the revenue contribution of homepage viewers to site-wide revenue (&#x00A3;173,287 / &#x00A3;547,524 = 31.6 per cent), rounded up to 32 per cent.</a></p>
</figcaption>
<p class="figure-media"><img src="images/M07NF002.jpg" alt="A site revenue page by Google Analytics compares revenues for all sessions and homepage views as 547, 524.47 pound sterling and 173, 287.05 pound sterling respectively."/></p>
</figure>
</section>
<section>
<h4 class="head-c">Effort</h4>
<p class="text-fo">Some experiments require more effort to bring to life than others. Effort increases cost, which drives down ROI. Effort can be technical or political in nature.</p>
<p class="text">Changing a line of copy is reasonably straightforward; someone with a very rudimentary knowledge of coding should be able to build this test. Other tests may require advanced front-end coding skills, visual design and specialist testing platform knowledge.</p>
<blockquote>
<p class="block-quote">More specialist skills &#x002B; More time = Increased cost = Lower ROI</p>
</blockquote>
<p class="text-fo">It&#x2019;s also worth considering the complexity and cost of deploying a winning test to the code base of the website or app.</p>
<p class="text"><span id="page-123" role="doc-pagebreak" aria-label=" Page 123. " epub:type="pagebreak"/>Political complexities are often harder to crack than technical ones. We know of a giant international brand that would occasionally exclude their company&#x2019;s internal IP address range from experiments to limit the chances of colleagues seeing the changes!</p>
<p class="text">Expect some pushback on certain ideas. Over time, as the programme builds credibility, resistance will ease. Even then, pick your battles wisely.</p>
<p class="text">The homepage is often especially sensitive. Different silos in the business all want their interest to be represented there. In some organizations it&#x2019;s sacrosanct. We worked with an optimization team at a well-known fashion retailer that believed their homepage was rich in potential. However, they were at odds with their colleagues in the marketing department who were concerned that testing would result in &#x2018;hard sell&#x2019; messages on the homepage diluting the brand.</p>
<p class="text">Some of our clients operate in a highly regulated environment. Whereas small copy changes are usually relatively stress free, in those organizations almost every phrase has to be checked by the legal department. This can introduce long delays and several iterations to get the copy legal-proof, by which time it may have lost its punch. Needless to say, these ideas score low on the grounds of effort.</p>
</section>
</section>
<section aria-labelledby="ch07_h3_002">
<h3 id="ch07_h3_002" class="head-b">Cost of delay</h3>
<p class="text-fo">In product development circles, <i>cost of delay</i> <i>(</i><i>CoD</i><i>)</i> is often used as a factor in prioritization. &#x2018;Being wrong may be less costly than you think,&#x2019; explains Jeff Bezos, &#x2018;whereas being slow is going to be expensive for sure.&#x2019;<sup><a href="#ch07_en3" id="ch07_enx3">3</a></sup> CoD is a measure of revenue loss or deferment as a result of delays.</p>
<p class="text">This practice is less common in optimization. One of the biggest risks for delay in optimization is around test duration. Sample sizes can be enlarged by extending the run of an experiment, thereby exposing it to a bigger population.</p>
<p class="text">A larger sample size is required to be able to detect smaller differences with confidence. So, a 3 per cent lift will take longer to detect than a 5 per cent lift, which in turn will take longer to detect than a 10 per cent lift. Bear in mind that most wins are modest.</p>
<p class="text">Increasing test duration for the sake of detecting that smaller effect is not always a good trade-off. There&#x2019;s an opportunity cost in extending the life of an experiment, as it occupies a testing slot. It could be making way for another experiment with a better chance of producing a more compelling win.</p>
<p class="text"><span id="page-124" role="doc-pagebreak" aria-label=" Page 124. " epub:type="pagebreak"/>The <i>minimum detectable effect</i> (MDE), which is the smallest lift that can be confidently detected in an experiment, can be calculated in advance.<sup><a epub:type="noteref" role="doc-noteref" href="#ch07_en4" id="ch07_enx4">4</a></sup> You can find MDE calculators online if your testing platform doesn&#x2019;t offer one. This will give you the <i>minimum required sample size</i> (MRSS). Then it&#x2019;s a simple matter of calculating how long a test would need to run based on visitor numbers for that testing slot.</p>
<p class="text">Here&#x2019;s a real example using Optimizely&#x2019;s MDE calculator: Detecting a 10 per cent increase on a given page requires 31,000 visitors per variation. That&#x2019;s a combined sample of 62,000 for the control and our variation. So if this page draws 62,000 visitors per month, it would take a month to detect a 10 per cent lift on that page. Reducing MDE from 10 per cent to 5 per cent blows sample size requirements out of the water. MRSS shoots up by more than 300 per cent to 140,000 per variation! It will take 4.5 months for this experiment to reach statistical significance, compared to only 1 month if we settle on 10 per cent MDE.</p>
<p class="text">It&#x2019;s not necessary to calculate MDE for each test you plan to run. If you know the MDE for a testing slot, consider the likelihood of a particular experiment meeting different MDE levels. </p>
</section>
</section>
<section aria-labelledby="ch07_h2_002">
<h2 id="ch07_h2_002" class="head-a">Prioritization frameworks</h2>
<p class="text-fo">There are a number of established prioritization frameworks that have become widely adopted in the industry. It&#x2019;s beyond the scope of this book to discuss each of them.</p>
<p class="text">What&#x2019;s more important than choice of framework is having a framework, and of course, using it. Below are two examples.</p>
<section aria-labelledby="ch07_h3_003">
<h3 id="ch07_h3_003" class="head-b">Value/Effort*Confidence</h3>
<p class="text-fo">Grounded in the Value/Effort principle discussed above, the framework shown in <a id="rch07_tab_001" href="#ch07_tab_001">Table 7.1</a> is easy to understand and implement. It&#x2019;s been endorsed by the highly regarded UX thought leader and prolific author Jared Spool<sup><a href="#ch07_en5" id="ch07_enx5">5</a></sup> (look him up on Twitter).</p>
<figure class="table" id="ch07_tab_001"><figcaption>
<p class="table"><a href="#rch07_tab_001"><span class="label" epub:type="label"><span id="page-125" role="doc-pagebreak" aria-label=" Page 125. " epub:type="pagebreak"/>Table</span> <span class="ordinal" epub:type="ordinal">7.1</span> Example of a prioritization framework, which can be set up in a spreadsheet with dropdowns for ease of use. The formula for Score is V/E*C.</a></p>
</figcaption><p class="table-skip"><a href="#ch07_skip_001">Skip table</a></p>
<!--<div class="table" role="table" tabindex="0">--><table class="shade-even">
<thead><tr>
<th scope="col"><p class="table-head"><b>Test Idea</b></p></th>
<th scope="col"><p class="table-head"><b>Value</b></p></th>
<th scope="col"><p class="table-head"><b>Effort</b></p></th>
<th scope="col"><p class="table-head"><b>Confidence</b></p></th>
<th scope="col"><p class="table-head"><b>Score</b></p></th>
</tr></thead>
<tbody><tr>
<td><p class="table-text">Test 1</p></td>
<td><p class="table-text">3</p></td>
<td><p class="table-text">3</p></td>
<td><p class="table-text">2</p></td>
<td><p class="table-text">2</p></td>
</tr>
<tr>
<td><p class="table-text">Test 2</p></td>
<td><p class="table-text">3</p></td>
<td><p class="table-text">2</p></td>
<td><p class="table-text">2</p></td>
<td><p class="table-text">3</p></td>
</tr>
<tr>
<td><p class="table-text">Test 3</p></td>
<td><p class="table-text">2</p></td>
<td><p class="table-text">2</p></td>
<td><p class="table-text">1</p></td>
<td><p class="table-text">1</p></td>
</tr></tbody></table><!--</div>--></figure>
<p class="text-fo" id="ch07_skip_001">For each idea, assign a value out of 3 for Value, Effort and Confidence. Set it up in a spreadsheet and format the cells so that you can select the appropriate value from a dropdown list. The formula for Score is V/E*C.<sup><a epub:type="noteref" role="doc-noteref" href="#ch07_en6" id="ch07_enx6">6</a></sup></p>
<p class="text">To make it more scientific and remove as much debate as possible, set rules to assign individual values. Here is an example of a basic rule set to quantify Effort:</p>
<ol class="list-number"><li><p class="list-number">Requires JavaScript coding; design resource needed</p></li>
<li><p class="list-number">Some HTML and/or CSS required</p></li>
<li><p class="list-number">No special coding or design needed.</p></li>
</ol>
<p class="text-fo">Effort can be quantified by asking engineers to assign a value for technical complexity, or by estimating total person hours.</p>
<p class="text">Here is an example of a more complex rubric, in this case to quantify Value (<a id="rch07_tab_002" href="#ch07_tab_002">Table 7.2</a>). Remember that one size doesn&#x2019;t fit all; adjust for your own situation.</p>
<figure class="table" id="ch07_tab_002"><figcaption>
<p class="table"><a href="#rch07_tab_002"><span class="label" epub:type="label">Table</span> <span class="ordinal" epub:type="ordinal">7.2</span> A sample rubric to quantify Value. Limit complexity in your framework as far as possible, but this is a good approach if you need more granularity. Remember to adapt this to your needs.</a></p>
</figcaption><p class="table-skip"><a href="#ch07_skip_002">Skip table</a></p>

<!--<div class="table" role="table" tabindex="0">--><table class="shade-even left">
<thead><tr>
<td><p class="table-head"/></td>
<th scope="col" class="center"><p class="table-head"><b>1</b></p><p class="table-head"><b>Low value</b></p></th>
<th scope="col" class="center"><p class="table-head"><b>2</b></p><p class="table-head"><b>Medium value</b></p></th>
<th scope="col" class="center"><p class="table-head"><b>3</b></p><p class="table-head"><b>High value</b></p></th>
</tr></thead>
<tbody><tr>
<td><p class="table-text"/><p class="table-text">Impact</p></td>
<td><p class="table-text">Test will have to rely on proxy metrics</p></td>
<td><p class="table-text">RPV not feasible, but another core metric can be used</p></td>
<td><p class="table-text">RPV (revenue per visitor) can be used as primary metric</p></td>
</tr>
<tr>
<td><p class="table-text"/><p class="table-text">MDE 5%</p><p class="table-text"/></td>
<td><p class="table-text">Test will run more than a month to detect 5% uplift</p></td>
<td><p class="table-text">Estimated test duration 1 month</p></td>
<td><p class="table-text">Confident result likely in 14 days</p></td>
</tr>
<tr>
<td><p class="table-text">Revenue multiplier</p></td>
<td><p class="table-text">Testing slot revenue multiplier of &#x003C;30%</p></td>
<td><p class="table-text">Testing slot revenue multiplier &#x003E;30% but &#x003C;80%</p></td>
<td><p class="table-text">Most revenue passes through testing slot; multiplier &#x003E;80%</p></td>
</tr></tbody></table><!--</div>--></figure>

<section>
<h4 class="head-c" id="ch07_skip_002"><span id="page-126" role="doc-pagebreak" aria-label=" Page 126. " epub:type="pagebreak"/>Confidence</h4>
<p class="text-fo">How confident are you that the experiment will have the desired outcome? Unless something really obvious is wrong on a page, it is impossible to know in advance.</p>
<p class="text">In fact, it goes somewhat against the spirit of prioritization, which is to remove personal intuition from the process. As Ronny Kohavi from Microsoft said: &#x2018;It is humbling to see how bad experts are at estimating the value of features&#x2026;.&#x2019;<sup><a href="#ch07_en7" id="ch07_enx7">7</a></sup></p>
<p class="text">One of the highest-earning experiments in the history of Bing, Microsoft&#x2019;s search engine, was deemed a low priority for months. An engineer made the call to launch it after realizing it would require relatively low effort to build. This experiment, which had been relegated to low-priority status for so long, added more than $100 million per annum in revenue in the United States alone.<sup><a epub:type="noteref" role="doc-noteref" href="#ch07_en8" id="ch07_enx8">8</a></sup> The result was so surprising that a team was assigned to do a sense check. Nobody could have predicted it.</p>
<p class="text">The famous PIE (Potential, Importance and Ease) framework has a similar dimension to Confidence. Potential refers to how much improvement you think can be made on a page. Chris Goward, who developed PIE, cautions that a &#x2018;certain amount of personal judgment and experience&#x2026; comes into play at this stage&#x2019;.<sup><a epub:type="noteref" role="doc-noteref" href="#ch07_en9" id="ch07_enx9">9</a></sup></p>
<p class="text">Goward suggests looking at signals like bounce rate, exit rate and usability issues during analysis to inform this assessment. Say, for example, that GA data reveal a massive drop-off rate on a page. During usability testing you discover that visitors on that page get stuck because a key piece of information about the product is missing. This would indicate high potential around the hypothesis that inserting the missing information on this page would increase sales.</p>
<p class="text">In our experience, another valuable data point for doing this is the outcome of previous A/B tests. Over time, you can get a sense of the inherent potential of each testing slot, as well as the types of intervention your users respond to.</p>
<p class="text">Here is a basic recipe that can be used to quantify Confidence:</p>
<ol class="list-number"><li><p class="list-number">We have no evidence to back this idea.</p></li>
<li><p class="list-number">There is some evidence from at least one source.</p></li>
<li><p class="list-number">Strong evidence from more than one data source.</p></li>
</ol>
<p class="text-fo">If you would like a more granular view, a sample rubric to quantify Confidence can be seen in <a id="rch07_tab_003" href="#ch07_tab_003">Table 7.3</a>.</p>
<figure class="table" id="ch07_tab_003"><figcaption>
<p class="table"><a href="#rch07_tab_003"><span class="label" epub:type="label"><span id="page-127" role="doc-pagebreak" aria-label=" Page 127. " epub:type="pagebreak"/>Table</span> <span class="ordinal" epub:type="ordinal">7.3</span> A sample rubric to quantify Confidence. Use this approach if you feel that you need more granularity in your decision-making</a></p>
</figcaption><p class="table-skip"><a href="#ch07_h3_004">Skip table</a></p>
<!--<div class="table" role="table" tabindex="0">--><table class="shade-even left">
<thead><tr>
<td><p class="table-head"/></td>
<th scope="col" class="center"><p class="table-head"><b>1</b></p><p class="table-head"><b>Low confidence</b></p></th>
<th scope="col" class="center"><p class="table-head"><b>2</b></p><p class="table-head"><b>Medium confidence</b></p></th>
<th scope="col" class="center"><p class="table-head"><b>3</b></p><p class="table-head"><b>High confidence</b></p></th>
</tr></thead>
<tbody><tr>
<td><p class="table-text"/><p class="table-text">Evidence</p></td>
<td><p class="table-text">We have no evidence to back this idea</p></td>
<td><p class="table-text">There is some evidence from at least one source</p></td>
<td><p class="table-text">Strong evidence from more than one data source supports idea</p></td>
</tr>
<tr>
<td><p class="table-text"/><p class="table-text">Page statistics</p><p class="table-text"/></td>
<td><p class="table-text">Bounce rate and/or exit rate on this page is above average</p></td>
<td><p class="table-text">Bounce rate and/or exit rate on this page is about average</p></td>
<td><p class="table-text">Page or template has a high bounce rate and/or exit rate</p></td>
</tr>
<tr>
<td><p class="table-text"/><p class="table-text">Usability</p></td>
<td><p class="table-text">Minor usability issue, or one with low occurrence rate</p></td>
<td><p class="table-text">Moderate usability issue or medium occurrence rate</p></td>
<td><p class="table-text">Severe usability issue, or one with high occurrence rate</p></td>
</tr>
<tr>
<td><p class="table-text">A/B test history</p></td>
<td><p class="table-text">Testing slot has not produced any confident wins</p></td>
<td><p class="table-text">Testing slot has produced only modest wins</p></td>
<td><p class="table-text">Testing slot has produced at least one strong, confident win</p></td>
</tr></tbody></table><!--</div>--></figure>
</section>
</section>
<section aria-labelledby="ch07_h3_004">
<h3 id="ch07_h3_004" class="head-b">The binary model</h3>
<p class="text-fo">Another approach is shown in <a id="rch07_tab_004" href="#ch07_tab_004">Table 7.4</a>. In this model, add 1 point if an idea satisfies the condition on the left, 0 points if it doesn&#x2019;t.<sup><a epub:type="noteref" role="doc-noteref" href="#ch07_en10" id="ch07_enx10">10</a></sup></p>
<figure class="table" id="ch07_tab_004"><figcaption>
<p class="table"><a href="#rch07_tab_004"><span class="label" epub:type="label"><span id="page-128" role="doc-pagebreak" aria-label=" Page 128. " epub:type="pagebreak"/>Table</span> <span class="ordinal" epub:type="ordinal">7.4</span> The binary model is an alternative method of prioritizing ideas. First, agree a list of considerations to put in the first column. Add 1 if it satisfies the condition, 0 if it doesn&#x2019;t.</a></p>
</figcaption><p class="table-skip"><a href="#ch07_skip_003">Skip table</a></p>
<!--<div class="table" role="table" tabindex="0">--><table class="shade-even left">
<thead><tr>
<th scope="col"><p class="table-head"><b>Consideration</b></p></th>
<th scope="col"><p class="table-head"><b>Add &#x002B;1 point if...</b></p></th>
<th scope="col"><p class="table-head"><b>Give 0 points if...</b></p></th>
</tr></thead>
<tbody><tr>
<td><p class="table-text">Primary metric = RPV</p></td>
<td><p class="table-text">It is feasible to track RPV as a primary metric</p></td>
<td><p class="table-text">RPV can&#x2019;t be supported as a primary metric</p></td>
</tr>
<tr>
<td><p class="table-text">Attrition rate</p></td>
<td><p class="table-text">Targets an area of the site that has a high drop-off rate</p></td>
<td><p class="table-text">Targets an area of the site that doesn&#x2019;t have high attrition</p></td>
</tr>
<tr>
<td><p class="table-text">Bounce rate / Exit rate</p></td>
<td><p class="table-text">The target page or template has a high bounce rate and/or exit rate</p></td>
<td><p class="table-text">Bounce rate and/or exit rate is average or below average</p></td>
</tr>
<tr>
<td><p class="table-text">Page value</p></td>
<td><p class="table-text">The target page or template has a relatively high page value</p></td>
<td><p class="table-text">Page value is average or below average</p></td>
</tr>
<tr>
<td><p class="table-text">Usability issue frequency</p></td>
<td><p class="table-text">The issue was seen frequently in usability testing</p></td>
<td><p class="table-text">The issue was observed once only, or not at all</p></td>
</tr>
<tr>
<td><p class="table-text">Usability issue severity</p></td>
<td><p class="table-text">The issue has the potential to get in the way of conversions</p></td>
<td><p class="table-text">Causes frustration, but does not block or hurt sales</p></td>
</tr>
<tr>
<td><p class="table-text">Test duration</p></td>
<td><p class="table-text">Test can be concluded in 14 days</p></td>
<td><p class="table-text">Test duration is likely to be more than 14 days</p></td>
</tr>
<tr>
<td><p class="table-text">Visibility</p></td>
<td><p class="table-text">Targets an area of the page which is visible to all visitors</p></td>
<td><p class="table-text">Scroll maps show the target area is not visible to everyone</p></td>
</tr>
<tr>
<td><p class="table-text">Effort</p></td>
<td><p class="table-text">The test is relatively easy to build and/or implement</p></td>
<td><p class="table-text">The test requires advanced coding skills and/or design</p></td>
</tr>
<tr>
<td><p class="table-text">Evidence</p></td>
<td><p class="table-text">Hypothesis is supported by at least two credible sources of evidence</p></td>
<td><p class="table-text">The idea is not backed by any credible evidence; it is based on intuition or opinion</p></td>
</tr></tbody></table><!--</div>--></figure>
<p class="text-fo" id="ch07_skip_003">Adapt this to your own needs and use your own labels. Don&#x2019;t be too zealous, though, as it can become a daunting exercise having to run through a long list of considerations. On the other hand, if you have too few points of reference in the table, you may end up with several ideas that have the same score.</p>
</section>
</section>
<section aria-labelledby="ch07_h2_003">
<h2 id="ch07_h2_003" class="head-a">The experimentation roadmap</h2>
<p class="text-fo">The <b>experimentation roadmap</b> forms your action plan, showing test hypotheses in order of priority. How to formulate the hypothesis is covered in <a href="14_chapter08.xhtml#ch08_sec_001">Chapter 8</a>.</p>
<p class="text">Simply pick the next one off the list when it&#x2019;s time to get a new test ready. The list is constantly updated and evaluated so that the highest ROI ideas are always at the top.</p>
<p class="text">As shown in <a id="rch07_tab_005" href="#ch07_tab_005">Table 7.5</a>, a roadmap should at least include the hypothesis, prioritization and current status, which is discussed in the next section. It&#x2019;s a good idea to include a column for testing slot, which in this example has been incorporated into the test id coding.</p>
<figure class="table" id="ch07_tab_005"><figcaption>
<p class="table"><a href="#rch07_tab_005"><span class="label" epub:type="label">Table</span> <span class="ordinal" epub:type="ordinal">7.5</span> A basic roadmap, including hypothesis, prioritization (columns marked V, E and C represent value, effort, confidence) and current status</a></p>
</figcaption><p class="table-skip"><a href="#ch07_skip_004">Skip table</a></p>
<!--<div class="table" role="table" tabindex="0">--><table class="default">
<thead><tr>
<th class="shade" scope="rowgroup" rowspan="2"><p class="table-head"><b>Test Id</b></p></th>
<th class="shade" scope="rowgroup" rowspan="2"><p class="table-head"><b>Hypothesis</b></p></th>
<th class="shade" scope="colgroup" colspan="4"><p class="table-head"><b>Prioritization</b></p></th>
<th class="shade" scope="rowgroup" rowspan="2"><p class="table-head"><b>Status</b></p></th>
</tr>
<tr>
<th class="shade" scope="col"><p class="table-head"><b>V</b></p></th>
<th class="shade" scope="col"><p class="table-head"><b>E</b></p></th>
<th class="shade" scope="col"><p class="table-head"><b>C</b></p></th>
<th class="shade" scope="col"><p class="table-head"><b>Score</b></p></th>
</tr></thead>
<tbody><tr>
<td><p class="table-text">HP001</p></td>
<td><p class="table-text"/></td>
<td><p class="table-text"/></td>
<td><p class="table-text"/></td>
<td><p class="table-text"/></td>
<td><p class="table-text"/></td>
<td><p class="table-text"/></td>
</tr>
<tr>
<td class="shade"><p class="table-text">PDP002</p></td>
<td class="shade"><p class="table-text"/></td>
<td class="shade"><p class="table-text"/></td>
<td class="shade"><p class="table-text"/></td>
<td class="shade"><p class="table-text"/></td>
<td class="shade"><p class="table-text"/></td>
<td class="shade"><p class="table-text"/></td>
</tr>
<tr>
<td><p class="table-text">PLP003</p></td>
<td><p class="table-text"/></td>
<td><p class="table-text"/></td>
<td><p class="table-text"/></td>
<td><p class="table-text"/></td>
<td><p class="table-text"/></td>
<td><p class="table-text"/></td>
</tr></tbody></table><!--</div>--></figure>
<p class="text-fo" id="ch07_skip_004"><span id="page-129" role="doc-pagebreak" aria-label=" Page 129. " epub:type="pagebreak"/>The roadmap should never be set in stone. Be flexible if new insights warrant a change to the original plan. New insights may come from a range of sources:</p>
<ul class="list-bullet"><li><p class="list-bullet">Insights from concluded experiments</p></li>
<li><p class="list-bullet">New data not previously available</p></li>
<li><p class="list-bullet">Previous research re-examined</p></li>
<li><p class="list-bullet">Changes in strategic priority</p></li>
<li><p class="list-bullet">fresh ideas from people in the organization.</p></li>
</ul>
<p class="text-fo">Another good practice is to aim for a balance of low-, medium- and high-complexity tests. This way you avoid the risk of missing good opportunities in the high value, high effort quadrant (see <a href="#ch07_fig_001">Figure 7.1</a>) by focusing only on low-effort experiments.</p>
<section aria-labelledby="ch07_h3_005">
<h3 id="ch07_h3_005" class="head-b">Keeping track of everything</h3>
<p class="text-fo">The more experiments you run, the more difficult it becomes to maintain visibility on all the moving parts. This increases the risk of expensive delays, communication failures and unmatched expectations.</p>
<p class="text">As Effective Experiments founder Manuel da Costa explains: &#x2018;Running an optimization programme is time consuming work with analysing data and running tests. We found that teams were becoming inefficient and unable to keep track of everything that was going on.&#x2019; An experienced optimizer himself, he created a system designed with experimentation workflows in mind.</p>
<p class="text">Just as with prioritization, which system is not as important as having a system. You may already have project management software. We&#x2019;ve used Jira, Basecamp, <a href="https://Monday.com"><span class="url-hyperlink">Monday.com</span></a>, Asana and Smartsheet very effectively. Alternatively, you can do it all for free with spreadsheets and Trello.</p>
<section>
<h4 class="head-c">Scheduling</h4>
<p class="text-fo">On high-velocity programmes especially, it&#x2019;s useful to have a plan that shows launch dates and even completion dates for each experiment. This helps to manage the pipeline of tests and make sure there&#x2019;s always something in the queue. A version of this can be seen in <a id="rch07_tab_006" href="#ch07_tab_006">Table 7.6</a>.</p>
<figure class="table" id="ch07_tab_006"><figcaption>
<p class="table"><a href="#rch07_tab_006"><span class="label" epub:type="label"><span id="page-130" role="doc-pagebreak" aria-label=" Page 130. " epub:type="pagebreak"/>Table</span> <span class="ordinal" epub:type="ordinal">7.6</span> Example of a slot-based schedule. Test 1 runs on the homepage for four weeks, and is replaced by Test 6 in the second month. The PDP has enough traffic to warrant a faster turnaround, so tests are replaced every 14 days.</a></p>
</figcaption><p class="table-skip"><a href="#ch07_skip_005">Skip table</a></p>
<!--<div class="table" role="table" tabindex="0">--><table class="shade-even left">
<thead><tr>
<th scope="col"><p class="table-head"><b>Timeframe</b></p></th>
<th scope="col"><p class="table-head"><b>Homepage</b></p></th>
<th scope="col"><p class="table-head"><b>PLP</b></p></th>
<th scope="col"><p class="table-head"><b>PDP</b></p></th>
<th scope="col"><p class="table-head"><b>Basket</b></p></th>
<th scope="col"><p class="table-head"><b>Sitewide</b></p></th>
</tr></thead>
<tbody><tr>
<td><p class="table-text"><b>Wk 1&#x2013;Wk 2</b></p></td>
<td><p class="table-text">Test 1</p></td>
<td><p class="table-text">Test 2</p></td>
<td><p class="table-text">Test 3</p></td>
<td><p class="table-text">Test 4</p></td>
<td><p class="table-text">Test 5</p></td>
</tr>
<tr>
<td><p class="table-text"><b>Wk 3&#x2013;Wk 4</b></p></td>
<td><p class="table-text">Test 1</p></td>
<td><p class="table-text">Test 7</p></td>
<td><p class="table-text">Test 8</p></td>
<td><p class="table-text">Test 4</p></td>
<td><p class="table-text">Test 11</p></td>
</tr>
<tr>
<td><p class="table-text"><b>Wk 1&#x2013;Wk 2</b></p></td>
<td><p class="table-text">Test 6</p></td>
<td><p class="table-text">Test 7</p></td>
<td><p class="table-text">Test 9</p></td>
<td><p class="table-text">Test 10</p></td>
<td><p class="table-text">Test 12</p></td>
</tr></tbody></table><!--</div>--></figure>
<p class="text-fo" id="ch07_skip_005">Don&#x2019;t plan too far into the future though. A good rule of thumb is a quarterly planning window, combined with a monthly review to ensure you&#x2019;re still working on the right stuff.</p>
</section>
<section>
<h4 class="head-c">Manage work-in-progress</h4>
<p class="text-fo">It&#x2019;s important to keep track of tests as they pass through the various stages from idea to conclusion.</p>
<p class="text"><i>Harvard Business Review</i> contrasted work-in-progress inventory in traditional factory environments with our world. In our environment &#x2018;inventory largely consists of information, such as design documentation, test procedures and results, and instructions for building prototypes&#x2019;.<sup><a epub:type="noteref" role="doc-noteref" href="#ch07_en11" id="ch07_enx11">11</a></sup> The recommendation is that these invisible inventory items should be given visibility.</p>
<p class="text">You can do this by using a Kanban board such as Trello, or simply marking the status column in <a href="#ch07_tab_005">Table 7.5</a>. Here are examples from our own programmes:</p>
<ul class="list"><li><p class="list"><b>JDI (Just Do It).</b> Some things are so obviously plain wrong, they just need to be fixed. These should be channelled to the relevant product team or go straight to the developers. Examples include browser and/or device compatibility issues and bugs.</p></li>
<li><p class="list"><b>Analysis.</b> Further analysis is needed to complete the hypothesis or inform a solution. Go back to the relevant data sources with specific research questions. See <a href="10_chapter04.xhtml#ch04_sec_001">Chapter 4</a>.</p></li>
<li><p class="list"><b>Creative.</b> A catch-all for wireframing, copywriting and visual assets. The first step in getting an idea ready for testing.</p></li>
<li><p class="list"><b>Dev.</b> A test spec has been submitted to the developers for coding.</p></li>
<li><p class="list"><b><span id="page-131" role="doc-pagebreak" aria-label=" Page 131. " epub:type="pagebreak"/>QA.</b> Coding has been completed and the test has been set up in the experimentation platform. Quality assessment is now being performed to ensure it&#x2019;s free of bugs and behaving as expected.</p></li>
<li><p class="list"><b>Queue.</b> A test is ready to be launched, pending approval or the conclusion of another test. To avoid losing any momentum, aim to have a test lined up for each slot.</p></li>
<li><p class="list"><b>Live.</b> The experiment is running.</p></li>
<li><p class="list"><b>Concluded.</b> The final bucket to indicate that a test has been completed.</p></li>
</ul>
</section>
</section>
</section>
<section aria-labelledby="ch07_h2_004">
<h2 id="ch07_h2_004" class="head-a">Summary</h2>
<p class="text-fo">The experimentation roadmap is a prioritized list of all your test ideas. Randomly moving from one test to another produces inferior results.</p>
<p class="text">Use a prioritization framework to decide which ideas to progress, and how to order them on the roadmap. Fundamentally, prioritization is about managing resources.</p>
<p class="text">The most basic framework is an impact&#x2013;effort matrix, but it&#x2019;s important to also consider the location of a test. Robust experimentation requires minimum sample sizes to be observed. Therefore, it makes sense to give priority to pages that see more traffic. You can increase the test population by clustering pages together and testing at a template level, rather than on individual pages.</p>
<p class="text">Some tests have direct line of sight to business KPIs like revenue and conversion rate. All things being equal, those tests should rank higher than ones where the link to KPIs may be more tenuous. This depends partly on the distance of your test to the final conversion point.</p>
<p class="text">Cost of delay is often overlooked in many prioritization frameworks. Since there is a finite number of testing slots on your site, there is an opportunity cost associated with each test. The longer a test occupies a testing slot, the higher that cost.</p>
<p class="text">To keep track of everything, use status labels to give visibility to work-in-progress items. There are bespoke tools that have been developed specifically for this purpose, but standard project management software works as well. Smaller programmes can be managed effectively using spreadsheets and Trello.</p>
<p class="text"><span id="page-132" role="doc-pagebreak" aria-label=" Page 132. " epub:type="pagebreak"/>Always allow for some flexibility with your roadmap. The list should be updated every time new insights come to light. It should also be reviewed regularly as priorities may have changed on the back of new information. Limit advance planning to a quarterly window, reviewed on a monthly basis to ensure that you&#x2019;re still working on the biggest opportunities.</p>
</section>
<section id="ch07_sec_002" class="chapter-endnotes" aria-labelledby="ch07_h2_005">
<h2 id="ch07_h2_005" class="title" epub:type="title">Notes</h2>
<ol class="none">
<li class="endnote-text" id="ch07_en1"><a role="doc-backlink"  href="#ch07_enx1">1</a> Diamandis, P (2016) [Accessed 6 August 2020] Culture &#x0026; experimentation &#x2013; with Uber&#x2019;s Chief Product Officer, <i>Medium</i> [Online] <a href="https://medium.com/abundance-insights/culture-experimentation-with-uber-s-chief-product-officer-520dc22cfcb4"><span class="url-hyperlink">https://medium.com/abundance-insights/culture-experimentation-with-uber-s-chief-product-officer-520dc22cfcb4</span></a> (archived at <a href="https://perma.cc/N652-G3PE"><span class="url-hyperlink">https://perma.cc/N652-G3PE</span></a>)</li>
<li class="endnote-text" id="ch07_en2"><a role="doc-backlink"  href="#ch07_enx2">2</a> BBC (2016) [Accessed 6 August 2020] Bill Gates, Desert Island Discs, <i>BBC</i> <i>R</i><i>adio 4</i> [Online] <a href="http://www.bbc.co.uk/programmes/b06z1zdt"><span class="url-hyperlink">www.bbc.co.uk/programmes/b06z1zdt</span></a> (archived at <a href="https://perma.cc/PS2F-C3Z5"><span class="url-hyperlink">https://perma.cc/PS2F-C3Z5</span></a>)</li>
<li class="endnote-text" id="ch07_en3"><a role="doc-backlink"  href="#ch07_enx3">3</a> Bezos, J (2016) [Accessed 6 August 2020] 2016 letter to shareholders, <i>About Amazon</i> [Online] <a href="https://blog.aboutamazon.com/company-news/2016-letter-to-shareholders"><span class="url-hyperlink">https://blog.aboutamazon.com/company-news/2016-letter-to-shareholders</span></a> (archived at <a href="https://perma.cc/XS8L-YZD3"><span class="url-hyperlink">https://perma.cc/XS8L-YZD3</span></a>)</li>
<li class="endnote-text" id="ch07_en4"><a role="doc-backlink"  href="#ch07_enx4">4</a> Bloom, H S (2008) The core analytics of randomized experiments for social research, in <i>The Sage</i> <i>H</i><i>andbook of</i> <i>Social Research Methods</i>, ed P Alasuutari, L Bickman and J Brannen, pp 115&#x2013;33, SAGE, London</li>
<li class="endnote-text" id="ch07_en5"><a role="doc-backlink"  href="#ch07_enx5">5</a> Spool, J (2019) [Accessed 6 August 2020] (Value &#x00F7; Effort) &#x00D7; Confidence = Priority, <i>Medium</i> [Online] <a href="https://medium.com/@jmspool/value-effort-x-confidence-priority-46dfad80f936"><span class="url-hyperlink">https://medium.com/&#x0040;jmspool/value-effort-x-confidence-priority-46dfad80f936</span></a> (archived at <a href="https://perma.cc/GQ9Z-2WFE"><span class="url-hyperlink">https://perma.cc/GQ9Z-2WFE</span></a>)</li>
<li class="endnote-text" id="ch07_en6"><a role="doc-backlink"  href="#ch07_enx6">6</a> Moreira, M E (2017) <i>The</i> <i>Agile Enterprise: Building</i> <i>and running agile organizations</i>, Apress, New York</li>
<li class="endnote-text" id="ch07_en7"><a role="doc-backlink"  href="#ch07_enx7">7</a> Kohavi, R <i>et al</i> (2009) Online experimentation at Microsoft, <i>Data Mining Case Studies</i>, 11, p 39</li>
<li class="endnote-text" id="ch07_en8"><a role="doc-backlink"  href="#ch07_enx8">8</a> Kohavi, R and Thomke, S H (2017) The surprising power of online experiments, <i>Harvard Business Review</i>, September&#x2013;October, pp 74&#x2013;82</li>
<li class="endnote-text" id="ch07_en9"><a role="doc-backlink"  href="#ch07_enx9">9</a> Goward, C (2013) <i>You</i> <i>Should Test That! Conversion optimization for more leads, sales</i> <i>and revenue</i> <i>or t</i><i>he art and science of optimized marketing</i>, John Wiley &#x0026; Sons, Hoboken, NJ</li>
<li class="endnote-text" id="ch07_en10"><a role="doc-backlink"  href="#ch07_enx10">10</a> Rusonis, S (2015) [Accessed 6 August 2020] A method for prioritizing A/B test ideas that won&#x2019;t hurt feelings, <i>Optimizely</i> [Online] <a href="https://blog.optimizely.com/2015/05/05/how-to-prioritize-ab-testing-ideas/"><span class="url-hyperlink">https://blog.optimizely.com/2015/05/05/how-to-prioritize-ab-testing-ideas/</span></a> (archived at <a href="https://perma.cc/ZAN7-GLQW"><span class="url-hyperlink">https://perma.cc/ZAN7-GLQW</span></a>)</li>
<li class="endnote-text" id="ch07_en11"><a role="doc-backlink"  href="#ch07_enx11">11</a> Thomke, S and Reinersten, D (2012) Six myths of product development, <i>Harvard Business Review</i>, <b>90</b> (5), pp 84&#x2013;94</li>
</ol>
</section>
</section></body>
</html>